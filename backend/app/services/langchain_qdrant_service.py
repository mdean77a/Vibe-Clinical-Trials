"""
LangChain-based Qdrant service for document storage and RAG operations.

This service provides high-level document operations using LangChain:
- Document chunking and embedding via LangChain
- Similarity search for RAG (Retrieval-Augmented Generation)
- LangChain retriever patterns for document generation workflows
- Integration with LangGraph for ICF and checklist generation

NOTE: For protocol metadata and collection management, use qdrant_service.py
which provides direct Qdrant client operations.
"""

import logging
import os
from typing import Any, Dict, List, Optional

from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from langchain_openai import OpenAIEmbeddings
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient

from ..config import EMBEDDING_MODEL
from .qdrant_service import get_qdrant_service

# Load environment variables for local development
try:
    from dotenv import load_dotenv

    load_dotenv()
except ImportError:
    pass

logger = logging.getLogger(__name__)


class LangChainQdrantError(Exception):
    """Exception raised for LangChain Qdrant-related errors."""

    pass


class LangChainQdrantService:
    """Service class for LangChain-integrated Qdrant operations."""

    def __init__(
        self,
        client: Optional[QdrantClient] = None,
        embeddings: Optional[OpenAIEmbeddings] = None,
    ):
        """Initialize LangChain Qdrant service with appropriate clients."""
        # Reuse Qdrant client from qdrant_service to avoid duplicate initialization
        if client:
            self.client = client
        else:
            # Get the shared Qdrant client from qdrant_service
            qdrant_service = get_qdrant_service()
            self.client = qdrant_service.client
            logger.info("Reusing Qdrant client from qdrant_service")

        # Initialize OpenAI embeddings
        if embeddings:
            self.embeddings = embeddings
        else:
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if openai_api_key:
                self.embeddings = OpenAIEmbeddings(  # type: ignore[call-arg]
                    model=EMBEDDING_MODEL,
                    openai_api_key=openai_api_key,
                )
                logger.info(
                    f"OpenAI embeddings initialized successfully with model: {EMBEDDING_MODEL}"
                )
            else:
                self.embeddings = None  # type: ignore[assignment]
                logger.warning("OpenAI API key not found - embeddings will not work")

    def get_vector_store(self, collection_name: str) -> QdrantVectorStore:
        """Get LangChain QdrantVectorStore for a specific collection."""
        if not self.embeddings:
            raise LangChainQdrantError("Embeddings not initialized")

        return QdrantVectorStore(
            client=self.client,
            collection_name=collection_name,
            embedding=self.embeddings,
        )

    def store_documents(
        self,
        documents: List[Document],
        study_acronym: str,
        ids: Optional[List[str]] = None,  # Kept for backward compatibility
    ) -> tuple[List[str], str]:
        """Store documents in a Qdrant collection using LangChain."""
        try:
            # Generate collection name using qdrant service
            qdrant_service = get_qdrant_service()
            collection_name = qdrant_service.generate_collection_name(study_acronym)
            logger.info(
                f"Generated collection name: {collection_name} for study: {study_acronym}"
            )

            # Use from_documents to create collection and store documents
            logger.info(
                f"Creating collection and storing {len(documents)} documents with embeddings model: {self.embeddings}"
            )
            QdrantVectorStore.from_documents(
                documents=documents,
                embedding=self.embeddings,
                collection_name=collection_name,
                client=self.client,  # Use existing client instead of url/api_key
            )

            # Get the document IDs (they're auto-generated by from_documents)
            doc_ids = [
                str(i) for i in range(len(documents))
            ]  # from_documents doesn't return IDs
            logger.info(
                f"Created collection and stored {len(documents)} documents in {collection_name}"
            )

            return doc_ids, collection_name
        except Exception as e:
            logger.error(f"Error storing documents in {collection_name}: {e}")
            raise LangChainQdrantError(f"Failed to store documents: {str(e)}")

    def get_retriever(
        self,
        collection_name: str,
        search_type: str = "similarity",
        search_kwargs: Optional[Dict[str, Any]] = None,
    ) -> BaseRetriever:
        """Get a LangChain retriever for a specific collection."""
        try:
            vector_store = self.get_vector_store(collection_name)

            if search_kwargs is None:
                search_kwargs = {"k": 10}

            return vector_store.as_retriever(
                search_type=search_type,
                search_kwargs=search_kwargs,
            )
        except Exception as e:
            logger.error(f"Error creating retriever for {collection_name}: {e}")
            raise LangChainQdrantError(f"Failed to create retriever: {str(e)}")

    def similarity_search(
        self,
        collection_name: str,
        query: str,
        k: int = 5,
        score_threshold: Optional[float] = None,
    ) -> List[Document]:
        """Perform similarity search using LangChain."""
        try:
            vector_store = self.get_vector_store(collection_name)

            if score_threshold is not None:
                # Get results with scores and filter by threshold
                results_with_scores = vector_store.similarity_search_with_score(
                    query, k=k
                )
                filtered_results = [
                    doc
                    for doc, score in results_with_scores
                    if score >= score_threshold
                ]
                return filtered_results
            else:
                return vector_store.similarity_search(query, k=k)
        except Exception as e:
            logger.error(f"Error in similarity search for {collection_name}: {e}")
            raise LangChainQdrantError(f"Failed to perform similarity search: {str(e)}")

    def similarity_search_with_score(
        self,
        collection_name: str,
        query: str,
        k: int = 5,
    ) -> List[tuple[Document, float]]:
        """Perform similarity search with scores using LangChain."""
        try:
            vector_store = self.get_vector_store(collection_name)
            results = vector_store.similarity_search_with_score(query, k=k)
            return results
        except Exception as e:
            logger.error(
                f"Error in similarity search with score for {collection_name}: {e}"
            )
            raise LangChainQdrantError(
                f"Failed to perform similarity search with score: {str(e)}"
            )

    def list_collections(self) -> List[str]:
        """List all collections."""
        try:
            collections = self.client.get_collections()
            return [col.name for col in collections.collections]
        except Exception as e:
            logger.error(f"Error listing collections: {e}")
            raise LangChainQdrantError(f"Failed to list collections: {str(e)}")

    def delete_collection(self, collection_name: str) -> bool:
        """Delete a collection."""
        try:
            self.client.delete_collection(collection_name=collection_name)
            logger.info(f"Deleted collection: {collection_name}")
            return True
        except Exception as e:
            logger.error(f"Error deleting collection {collection_name}: {e}")
            return False

    def get_collection_info(self, collection_name: str) -> Optional[Dict[str, Any]]:
        """Get information about a collection."""
        try:
            collection_info = self.client.get_collection(collection_name)
            return {
                "name": collection_name,
                "points_count": collection_info.points_count,
                "vectors_count": collection_info.vectors_count,
                "status": collection_info.status,
            }
        except Exception as e:
            logger.error(f"Error getting collection info for {collection_name}: {e}")
            return None

    def test_connection(self) -> bool:
        """Test Qdrant connection."""
        # Delegate to qdrant_service to avoid duplication
        qdrant_service = get_qdrant_service()
        return qdrant_service.test_connection()


# Cache for service instance to avoid repeated initialization
_langchain_service_instance = None


def get_langchain_qdrant_service() -> LangChainQdrantService:
    """Get configured LangChain Qdrant service instance (singleton pattern)."""
    global _langchain_service_instance
    if _langchain_service_instance is None:
        _langchain_service_instance = LangChainQdrantService()
    return _langchain_service_instance
