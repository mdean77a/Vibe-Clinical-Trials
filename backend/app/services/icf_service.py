"""
ICF (Informed Consent Form) generation service.

This module provides high-level services for generating ICF documents
using LangGraph workflows and RAG context retrieval.
"""

import asyncio
import logging
from queue import Queue
from typing import Any, Dict, List, Optional

from qdrant_client import QdrantClient

from ..config import get_llm_chat_model
from ..prompts.generation_prompts import SECTION_GENERATION_PROMPT
from .document_generator import (
    DocumentGenerationError,
    DocumentGenerator,
    StreamingICFWorkflow,
    get_langgraph_workflow,
)
from .qdrant_service import get_qdrant_service

logger = logging.getLogger(__name__)


class ICFGenerationService:
    """Service for generating ICF documents using LangGraph workflows."""

    def __init__(self, qdrant_client: Optional[QdrantClient] = None):
        """Initialize the ICF generation service."""
        self.qdrant_client = qdrant_client or get_qdrant_service().client
        self.document_generator = DocumentGenerator(self.qdrant_client)

        # LLM configuration - uses default config from get_llm_chat_model
        self.llm_config: Dict[str, Any] = {}

    async def generate_icf_streaming(  # type: ignore[no-untyped-def]
        self,
        protocol_collection_name: str,
        protocol_metadata: Optional[Dict[str, Any]] = None,
        sections_filter: Optional[List[str]] = None,
    ):
        """
        Generate ICF sections using LangGraph with streaming tokens from each node.

        Each section is generated by a separate LangGraph node in parallel, and this
        method streams tokens from each LLM call to the frontend in real-time.

        Args:
            protocol_collection_name: The Qdrant collection name for the protocol
            protocol_metadata: Optional metadata about the protocol
            sections_filter: Optional list of section names to generate. If None, generates all sections.

        Yields:
            Dict with streaming events from parallel section generation
        """
        import asyncio
        import threading
        import time
        from asyncio import Queue
        from queue import Queue as ThreadQueue

        try:
            # Start timing the generation process
            generation_start_time = time.time()
            logger.info(
                f"🚀 Starting streaming ICF generation for collection: {protocol_collection_name} at {time.strftime('%H:%M:%S')}"
            )

            # Create a thread-safe queue that can be accessed from both async and sync contexts
            thread_queue: ThreadQueue[Dict[str, Any]] = ThreadQueue()
            event_queue: Queue[Dict[str, Any]] = Queue()

            # Start a background task to transfer events from thread queue to async queue
            async def queue_transfer() -> None:
                """Transfer events from thread-safe queue to async queue."""
                while True:
                    try:
                        # Check for events from the workflow thread (non-blocking)
                        try:
                            event = thread_queue.get_nowait()
                            await event_queue.put(event)
                        except:
                            # No events available, yield control briefly
                            await asyncio.sleep(0.01)
                    except Exception as e:
                        logger.error(f"Queue transfer error: {e}")
                        break

            transfer_task = asyncio.create_task(queue_transfer())

            # Get the current event loop to pass to the workflow
            main_loop = asyncio.get_running_loop()

            # Create a modified ICF workflow that supports streaming
            streaming_workflow = StreamingICFWorkflow(
                self.llm_config,
                thread_queue,  # Use thread-safe queue
                self.document_generator,
                protocol_collection_name,
                main_loop,
                sections_filter,
            )

            # Prepare workflow inputs like the original implementation
            context = self.document_generator.get_protocol_context(
                protocol_collection_name,
                "informed consent form requirements eligibility procedures risks benefits",
            )

            if not context:
                logger.warning(
                    f"No context found for collection: {protocol_collection_name}"
                )
                context = [
                    {"text": "No specific protocol context available", "score": 0.0}
                ]

            logger.info(f"Using collection name: '{protocol_collection_name}'")

            # Initialize state with empty lists for all sections
            workflow_inputs: Dict[str, Any] = {
                "summary": [],
                "background": [],
                "participants": [],
                "procedures": [],
                "alternatives": [],
                "risks": [],
                "benefits": [],
            }

            # Start the workflow execution in a background task
            workflow_task = asyncio.create_task(
                self._execute_streaming_workflow(
                    streaming_workflow, workflow_inputs, event_queue
                )
            )

            # Stream events as they arrive from the queue
            sections_completed = 0
            total_sections = (
                len(sections_filter) if sections_filter else 7
            )  # ICF has 7 sections by default

            while sections_completed < total_sections:
                try:
                    # Wait for the next event with a timeout
                    event = await asyncio.wait_for(event_queue.get(), timeout=60.0)

                    if event["type"] == "section_complete":
                        sections_completed += 1
                    elif event["type"] == "error":
                        sections_completed = total_sections  # Stop on error

                    yield event

                except asyncio.TimeoutError:
                    logger.error("Timeout waiting for streaming events")
                    yield {
                        "type": "error",
                        "error": "Generation timeout - LLM taking too long to respond",
                    }
                    break

            # Wait for workflow to complete and get final results
            try:
                await asyncio.wait_for(workflow_task, timeout=10.0)
            except asyncio.TimeoutError:
                logger.warning("Workflow cleanup timeout")
            finally:
                # Stop the queue transfer task
                transfer_task.cancel()
                try:
                    await transfer_task
                except asyncio.CancelledError:
                    pass

            # Calculate total generation time
            generation_end_time = time.time()
            total_time = generation_end_time - generation_start_time

            # Log the completion with timing information
            logger.info(f"⏱️  ICF GENERATION COMPLETE!")
            logger.info(f"📊 Collection: {protocol_collection_name}")
            logger.info(
                f"⏰ Total Time: {total_time:.2f} seconds ({total_time/60:.1f} minutes)"
            )
            logger.info(f"📝 Sections: {sections_completed}/{total_sections} completed")
            logger.info(f"🏁 Finished at: {time.strftime('%H:%M:%S')}")

            # Send completion event
            yield {
                "type": "complete",
                "total_sections": total_sections,
                "completed_sections": sections_completed,
                "generation_time_seconds": round(total_time, 2),
                "errors": [],
            }

            logger.info(
                f"Streaming ICF generation completed for collection: {protocol_collection_name}"
            )

        except Exception as e:
            logger.error(f"Streaming ICF generation failed: {e}")
            yield {"type": "error", "error": f"Generation failed: {str(e)}"}

    async def _execute_streaming_workflow(self, workflow, inputs, event_queue) -> Any:  # type: ignore[no-untyped-def]
        """Execute the streaming workflow in the background."""
        try:
            # Use async invoke to allow events to be processed concurrently
            result = await asyncio.get_running_loop().run_in_executor(
                None, workflow.invoke, inputs
            )
            return result
        except Exception as e:
            await event_queue.put(
                {"type": "error", "error": f"Workflow execution failed: {str(e)}"}
            )

    def _generate_section_streaming_sync(  # type: ignore[no-untyped-def]
        self,
        protocol_collection_name: str,
        section_name: str,
        protocol_metadata: Optional[Dict[str, Any]] = None,
    ):
        """
        Generate a single ICF section with token-by-token streaming.

        This is a synchronous method that returns an async generator for streaming.

        Args:
            protocol_collection_name: The Qdrant collection name for the protocol
            section_name: Name of the section to generate
            protocol_metadata: Optional metadata about the protocol

        Returns:
            Async generator yielding streaming tokens
        """

        async def stream_section() -> Any:
            try:
                # Use the same section queries as initial generation for consistency
                # Get query from document generator to maintain consistency with workflow
                query = self.document_generator._get_section_query(section_name)  # type: ignore[attr-defined]
                context = self.document_generator.get_protocol_context(
                    protocol_collection_name, query
                )

                if not context:
                    context = [
                        {
                            "text": f"No specific context available for {section_name}",
                            "score": 0.0,
                        }
                    ]

                # Generate the section with streaming using LangChain's streaming
                async for chunk in self._generate_section_with_streaming_llm(
                    section_name=section_name,
                    context="\n\n".join([item.get("text", "") for item in context]),
                    section_prompt=self._get_section_prompt(section_name),
                ):
                    yield chunk

            except Exception as e:
                logger.error(f"Failed to stream section {section_name}: {e}")
                yield {
                    "type": "error",
                    "error": f"Failed to generate {section_name}: {str(e)}",
                }

        return stream_section()

    async def _generate_section_with_streaming_llm(  # type: ignore[no-untyped-def]
        self, section_name: str, context: str, section_prompt: str
    ):
        """
        Generate a section using the LLM with token streaming.

        Args:
            section_name: Name of the section being generated
            context: Protocol context for the section
            section_prompt: Section-specific prompt

        Yields:
            Dict with streaming tokens
        """
        try:
            from langchain_core.messages import HumanMessage, SystemMessage

            messages = [
                SystemMessage(content=section_prompt),
                HumanMessage(
                    content=SECTION_GENERATION_PROMPT.format(
                        context=context, section_name=section_name
                    )
                ),
            ]

            # Stream tokens from the LLM
            llm = get_llm_chat_model()
            async for chunk in llm.astream(messages):
                if hasattr(chunk, "content") and chunk.content:
                    yield {"type": "token", "content": chunk.content}

        except Exception as e:
            logger.error(f"Failed to stream {section_name} section: {e}")
            yield {
                "type": "error",
                "error": f"Failed to generate {section_name}: {str(e)}",
            }

    def _format_context_for_llm(self, context: List[Dict[str, Any]]) -> str:
        """Format context for LLM consumption."""
        if not context:
            return "No specific protocol context available."

        formatted = []
        for item in context:
            text = item.get("text", "")
            score = item.get("score", 0)
            formatted.append(f"[Relevance: {score:.2f}] {text}")

        return "\n\n".join(formatted)

    def _get_section_prompt(self, section_name: str) -> str:
        """Get the prompt for a specific ICF section."""
        from ..prompts.icf_prompts import ICF_PROMPTS

        return ICF_PROMPTS.get(
            section_name, "Generate an appropriate ICF section based on the context."
        )

    def get_generation_status(self, task_id: str) -> Dict[str, Any]:
        """
        Get the status of an ICF generation task.

        Note: This is a placeholder for future async task tracking.
        Currently returns a simple status since generation is synchronous.
        """
        return {
            "task_id": task_id,
            "status": "not_implemented",
            "message": "Task tracking not yet implemented",
        }

    async def validate_collection_exists(self, collection_name: str) -> bool:
        """
        Validate that a protocol collection exists in Qdrant.

        Args:
            collection_name: The collection name to validate

        Returns:
            True if collection exists, False otherwise
        """
        try:
            # Check if collection exists
            collections = self.qdrant_client.get_collections()
            collection_names = [c.name for c in collections.collections]

            exists = collection_name in collection_names
            logger.info(f"Collection {collection_name} exists: {exists}")
            return exists

        except Exception as e:
            logger.error(f"Failed to validate collection {collection_name}: {e}")
            return False

    async def get_protocol_summary(self, collection_name: str) -> Dict[str, Any]:
        """
        Get a summary of the protocol from the collection metadata.

        Args:
            collection_name: The collection name to get summary for

        Returns:
            Dict containing protocol summary information
        """
        try:
            # Get some sample content from the collection to create a summary
            search_results = self.qdrant_client.scroll(
                collection_name=collection_name, limit=5, with_payload=True
            )

            if not search_results[0]:  # No points found
                return {
                    "collection_name": collection_name,
                    "status": "empty",
                    "message": "No protocol content found",
                }

            # Extract metadata from the first point
            first_point = search_results[0][0]
            payload = first_point.payload

            summary = {
                "collection_name": collection_name,
                "status": "ready",
                "protocol_metadata": {
                    "title": (
                        payload.get("protocol_title", "Unknown")
                        if payload
                        else "Unknown"
                    ),
                    "filename": (
                        payload.get("filename", "Unknown") if payload else "Unknown"
                    ),
                    "document_id": (
                        payload.get("document_id", "Unknown") if payload else "Unknown"
                    ),
                    "total_chunks": len(search_results[0]),
                },
            }

            logger.info(f"Retrieved protocol summary for {collection_name}")
            return summary

        except Exception as e:
            logger.error(f"Failed to get protocol summary for {collection_name}: {e}")
            return {
                "collection_name": collection_name,
                "status": "error",
                "message": str(e),
            }


# Singleton service instance
_icf_service_instance: Optional[ICFGenerationService] = None


def get_icf_service() -> ICFGenerationService:
    """Get the singleton ICF generation service instance."""
    global _icf_service_instance
    if _icf_service_instance is None:
        _icf_service_instance = ICFGenerationService()
    return _icf_service_instance
